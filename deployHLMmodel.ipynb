{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyPart(data):\n",
    "    Xdata = data.iloc[:,0:177]\n",
    "    Ydata = []\n",
    "    for elem in data.iloc[:,177]:\n",
    "        if elem == \"Low\":\n",
    "            Ydata.append(0)\n",
    "        elif elem == \"High\":\n",
    "            Ydata.append(1)\n",
    "    Ydata = np.array(Ydata)\n",
    "    return Xdata, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rdk.fr_C_O_noCOO', 'rdk.Chi4v', 'rdk.fr_Ar_COO', 'rdk.fr_SH',\n",
      "       'rdk.Chi4n', 'rdk.SMR_VSA10', 'rdk.fr_para_hydroxylation',\n",
      "       'rdk.fr_barbitur', 'rdk.fr_halogen', 'rdk.fr_dihydropyridine',\n",
      "       ...\n",
      "       'rdk.PEOE_VSA9', 'rdk.fr_aldehyde', 'rdk.fr_pyridine',\n",
      "       'rdk.fr_tetrazole', 'rdk.RingCount', 'rdk.fr_nitro_arom_nonortho',\n",
      "       'rdk.Chi0v', 'rdk.fr_ArN', 'rdk.NumRotatableBonds', 'c#ClappClass'],\n",
      "      dtype='object', length=178)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "data = pd.read_csv('HLM\\HLMdesc.txt', sep=\"\\t\")\n",
    "data = data.drop(columns = ['Protocol Number', 'Experiment Start Time', 'Experiment Stop Time', 'Result Type Name', 'Result Operator', 'Result Number Rounded',\\\n",
    " 'Project NumName', 'Smiles', 'Leonumber', 'DResult Text', 'DResult Text_1'])\n",
    "data = data.drop(columns = ['Protocol Number_1', 'Experiment Start Time_1', 'Experiment Stop Time_1', 'Result Type Name_1', 'Result Operator_1', 'Result Number Rounded_1', 'Project NumName_1',\\\n",
    "       'Leonumber_1', 'origSmiles_1', 'ClappClass_1'])\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rdk.fr_C_O_noCOO', 'rdk.Chi4v', 'rdk.fr_Ar_COO', 'rdk.fr_SH',\n",
      "       'rdk.Chi4n', 'rdk.SMR_VSA10', 'rdk.fr_para_hydroxylation',\n",
      "       'rdk.fr_barbitur', 'rdk.fr_halogen', 'rdk.fr_dihydropyridine',\n",
      "       ...\n",
      "       'rdk.MolMR', 'rdk.PEOE_VSA9', 'rdk.fr_aldehyde', 'rdk.fr_pyridine',\n",
      "       'rdk.fr_tetrazole', 'rdk.RingCount', 'rdk.fr_nitro_arom_nonortho',\n",
      "       'rdk.Chi0v', 'rdk.fr_ArN', 'rdk.NumRotatableBonds'],\n",
      "      dtype='object', length=177)\n"
     ]
    }
   ],
   "source": [
    "# Seperate test set\n",
    "train = data.sample(frac=0.8,random_state=0)\n",
    "test = data.drop(train.index)\n",
    "#print(len(train))\n",
    "#print(len(test))\n",
    "Xtrain, Ytrain = xyPart(train)\n",
    "Xtest, Ytest = xyPart(test)\n",
    "print(Xtrain.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "data_stats = Xtrain.describe()\n",
    "data_stats = data_stats.transpose()\n",
    "def norm(x):\n",
    "  return (x - data_stats['mean']) / data_stats['std']\n",
    "nXtrain = norm(Xtrain)\n",
    "nXtest = norm(Xtest)\n",
    "#print(Xtrain.values[0])\n",
    "#print(nXtrain.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rm columns containing NA\n",
    "allCol = nXtrain.columns\n",
    "ncXtrain = nXtrain.dropna(axis='columns')\n",
    "col = ncXtrain.columns  # After removing NA cols\n",
    "delCol = []\n",
    "for c in allCol:\n",
    "    if c not in col:\n",
    "        delCol.append(c)\n",
    "ncXtest = nXtest.drop(delCol, axis=1)\n",
    "#print(nXtrain.values[0])\n",
    "#print(ncXtrain.values[0])\n",
    "#print(ncXtest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgwdk\\AppData\\Local\\Continuum\\miniconda3\\envs\\myAzureEnv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "learner = LogisticRegression()\n",
    "model = learner.fit(ncXtrain, Ytrain)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(ncXtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCA(pred, Ytest):\n",
    "    true = false = 0\n",
    "    for idx in range(len(pred)):\n",
    "        if Ytest[idx] == pred[idx]:\n",
    "            true = true + 1\n",
    "        else:\n",
    "            false = false + 1\n",
    "    print(true, false)        \n",
    "    print(\"CA \", true/float(true+false))\n",
    "    CA = true/float(true+false)\n",
    "    return CA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model\n",
      "1222 259\n",
      "CA  0.8251181634031061\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model\")\n",
    "getCA(pred, Ytest)\n",
    "\n",
    "import pickle\n",
    "fid = open(\"modelDump.pkl\", \"wb\")\n",
    "modelFid = pickle.dump(model, fid)\n",
    "fid.close()\n",
    "#modelFid = pickle.dumps(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "1222 259\n",
      "CA  0.8251181634031061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8251181634031061"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid = open(\"modelDump.pkl\", \"rb\")\n",
    "model2 = pickle.load(fid)\n",
    "fid.close()\n",
    "#model2 = pickle.loads(modelFid)\n",
    "pred2 = model2.predict(ncXtest)\n",
    "print(\"Loaded model\")\n",
    "getCA(pred2, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\jgwdk\\Projects\\AzureTutorial\\aml_config\\config.json\n",
      "ML_ws_jgwdk\twesteurope\teuw-ml-test-rg\twesteurope\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=\"hlmexp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mlworkspace.azure.ai/portal/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourceGroups/euw-ml-test-rg/providers/Microsoft.MachineLearningServices/workspaces/ML_ws_jgwdk/experiments/HLMexp\n",
      "Experiment#HLMexp\n",
      "/experiments/{0}\n",
      "HLMexp\n",
      "<function Experiment.from_directory at 0x000001DC73FD1598>\n"
     ]
    }
   ],
   "source": [
    "print(exp.get_portal_url())\n",
    "print(exp.identity)\n",
    "print(exp.EXPERIMENT_PATH)\n",
    "print(exp.name)\n",
    "print(exp.from_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DELIM', 'EXPERIMENT_PATH', 'PORTAL_URL', 'WORKSPACE_FMT', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_experiment_url', '_get_base_info_dict', '_identity', '_log_context', '_logger', '_name', '_prompt_message', '_repr_html_', '_serialize_to_dict', '_workspace', '_workspace_url', 'from_directory', 'get_docs_url', 'get_portal_url', 'get_runs', 'identity', 'list', 'name', 'start_logging', 'submit', 'workspace', 'workspace_object']\n"
     ]
    }
   ],
   "source": [
    "print(dir(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourceGroups/euw-ml-test-rg/providers/Microsoft.MachineLearningServices/workspaces/ML_ws_jgwdk',\n",
       " 'name': 'ML_ws_jgwdk',\n",
       " 'location': 'westeurope',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'workspaceid': '84aa8fd1-ddb1-45f3-ab24-51f645ae2097',\n",
       " 'description': '',\n",
       " 'friendlyName': '',\n",
       " 'creationTime': '2019-01-31T08:52:13.8541198+00:00',\n",
       " 'containerRegistry': '/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourcegroups/euw-ml-test-rg/providers/microsoft.containerregistry/registries/mlwsjgwdk5062499165',\n",
       " 'keyVault': '/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourcegroups/euw-ml-test-rg/providers/microsoft.keyvault/vaults/mlwsjgwdk4598696655',\n",
       " 'applicationInsights': '/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourcegroups/euw-ml-test-rg/providers/microsoft.insights/components/mlwsjgwdk9665417812',\n",
       " 'identityPrincipalId': 'e28dde4c-cff1-4736-94ff-053f7bd07c4f',\n",
       " 'identityTenantId': 'd78f7362-832c-4715-8e12-cc7bd574144c',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/f7ae3eeb-3ca0-42a2-b9e2-34c2480a9c1b/resourcegroups/euw-ml-test-rg/providers/microsoft.storage/storageaccounts/mlwsjgwdk8198101817'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-02-07T09:44:26.337000+00:00', 'creationTime': '2019-02-07T09:44:05.353545+00:00', 'currentNodeCount': 0, 'errors': None, 'modifiedTime': '2019-02-07T09:44:40.612650+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 0, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# Create resources\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = \"AzureCluster\"\n",
    "compute_min_nodes = 0\n",
    "compute_max_nodes = 4\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in Azure dir\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "fid = open(\"outputs\\modelDump.pkl\", \"wb\")\n",
    "modelFid = pickle.dump(model, fid)\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model HLMmodel_v2\n"
     ]
    }
   ],
   "source": [
    "# Register model\n",
    "from azureml.core.model import Model\n",
    "model = Model.register(model_path = \"outputs\\modelDump.pkl\",\n",
    "                       model_name = \"HLMmodel_v2\",\n",
    "                       tags = {\"key\": \"0.1\"},\n",
    "                       description = \"test registration\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import things required by this script\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# load the model\n",
    "def init():\n",
    "    global model\n",
    "    fid = open(\"outputs\\modelDump.pkl\", \"rb\")\n",
    "    model = pickle.load(fid)\n",
    "    fid.close()\n",
    "\n",
    "# Passes data to the model and returns the prediction\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    y_hat = model.predict(data)\n",
    "    return json.dumps(y_hat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WS object\n",
    "from azureml.core.webservice import AciWebservice\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {\"data\": \"HLM\", \"type\": \"classification\"}, \n",
    "                                               description = 'HLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# Image configuration\n",
    "image_config = ContainerImage.image_configuration(execution_script = \"scoreHLM.py\",\n",
    "                                                 runtime = \"python\",\n",
    "                                                 conda_file = \"myAzureEnv.yml\",\n",
    "                                                 description = \"HLM class model\",\n",
    "                                                 tags = {\"data\": \"HLM\", \"type\": \"class\"}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Image creation operation finished for image hlmwebservice:1, operation \"Succeeded\"\n",
      "Creating service\n",
      "Running........................\n",
      "FailedACI service creation operation finished, operation \"Failed\"\n",
      "Service creation polling reached terminal state, current service state: Unhealthy\n",
      "{\n",
      "  \"code\": \"AciDeploymentFailed\",\n",
      "  \"message\": \"Aci Deployment failed\",\n",
      "  \"details\": [\n",
      "    {\n",
      "      \"code\": \"CrashLoopBackOff\",\n",
      "      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance hlmwebservice.\\nYou can also try to run image mlwsjgwdk5062499165.azurecr.io/hlmwebservice:1 locally. Please refer to http://aka.ms/debugimage for more information.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# Create the webservice using all of the precreated configurations and our best model\n",
    "service = Webservice.deploy_from_model(name='hlmwebservice',\n",
    "                                       deployment_config = aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config,\n",
    "                                       workspace=ws)\n",
    "\n",
    "# Wait for the service deployment to complete while displaying log output\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to test prediction\n",
    "fid = open(\"ncXtest.pkl\", \"wb\")\n",
    "modelFid = pickle.dump(ncXtest, fid)\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "fid = open(\"ncXtest.pkl\", \"rb\")\n",
    "testData = pickle.load(fid)\n",
    "fid.close()\n",
    "#print(testData.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
